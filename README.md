# course-project-03-ASSEMBLY


# Project ASSEMBLY ‚Äî Energy-Aware Adaptive Batch-Level Scheduler for IIoT Fog‚ÄìCloud Environments

> **Core Contribution:** A **two-tier scheduler** combining:
- **BDRL-PPO (Bayesian Tiny Fast-Adaptive Proximal Policy Optimiser)** for high-level resource partitioning between fog and cloud.
- **ABS (Adaptive Batch Scheduler)** based on **Genetic Programming** for fine-grained task scheduling.

The system aims to **minimize energy consumption** while ensuring that batch workflows meet their soft deadlines (**QoS compliance**).

---

## 1. Motivation

Industrial IoT (IIoT) systems often run **batch workflows** during off-peak hours or shifts. These workflows can consist of **100s to 1000s of parallel tasks** that process large sensor logs and produce quality reports.

While these tasks are not real-time, their **energy cost** is significant due to their scale. Traditional **static heuristics** fail to adapt to changing conditions like workload size and node availability.

**ASSEMBLY** solves this with a **learn-and-adapt** approach:
- **BDRL-PPO** learns and adapts the **fog vs. cloud allocation ratio** quickly.
- **ABS** evolves a **fine-grained schedule** respecting BDRL's guidance.

---

## 2. Formal Problem Definition

### 2.1 System Model

- **Nodes:** Set of **fog nodes** (ùë≠) and **cloud nodes** (ùë™). Each has:
  - CPU speed *S‚Çô* [MIPS]
  - Idle & busy power: *P·µ¢‚Çô*, *P_b‚Çô* [W]
  - DVFS levels: *L‚Çô*
- **Network:** Bandwidth *B‚Çô‚Çò* and latency *RTT‚Çô‚Çò* between any pair
- **Workload:** DAG *G = (V,E)* where:
  - *w·µ•*: Task computation (MI)
  - *e·µ•*: Input data size (MB)
- **Deadline:** Soft deadline Œî·¥Æ for entire batch

### 2.2 Decision Variables

| Variable | Domain | Meaning |
|--------|--------|---------|
| *œÅ* | [0,1] | Fog allocation ratio from BDRL |
| *(x·µ•,t·µ•)* | (node, slot) | ABS assigns task *v* to node *x·µ•* at time *t·µ•* |
| *l·µ•* | DVFS level | Chosen for executing task *v* |

### 2.3 Objective

Minimize total energy **E_total**, including compute and network costs, subject to meeting the deadline:

$$
\min_{œÅ,x,t,l}\;E_\text{total}=\sum_{v‚ààV}\Big(P_b^{x_v}(l_v)¬∑\frac{w_v}{S_{x_v}(l_v)}+\sum_{(v,u)‚ààE}\frac{e_{vu}}{B_{x_vx_u}}¬∑P_\text{net}\Big)\]

Plus a penalty if the makespan exceeds the deadline:
$$
Œª¬∑\max(0,\text{finish}-Œî^B)
$$

---

## 3. Bayesian DRL Component (BDRL-PPO)

### 3.1 Why Bayesian?

Standard PPO models have fixed weights that don‚Äôt adapt well to new regimes. In contrast, **Bayesian PPO** treats weights as Gaussian distributions (mean + variance), allowing:

- **Uncertainty estimation**: High variance means uncertain decisions (useful for adaptation).
- **Few-shot adaptation**: One gradient update after a small rollout adapts the policy fast.

This allows **tiny model size (<200KB)** and **fast retraining** (under 10ms on edge devices).

### 3.2 States, Actions, Rewards

| Symbol | Shape | Description |
|--------|-------|-------------|
| *s‚Çú* | 7-D vector | ‚ü®pending_tasks, Fog_CPU%, Cloud_CPU%, avg_DVFS_level, SoC_battery, mean_RTT, time_to_deadline‚ü© |
| *a‚Çú=œÅ‚Çú* | scalar | Fraction of next H tasks to prefer fog |
| *r‚Çú* | scalar | Reward: ‚àíŒ±¬∑E_window ‚àí Œ≤¬∑lateness_window |

Where *H* = planning horizon (~50 tasks). *E_window* is total energy used in the current window. *lateness_window* penalizes missed deadlines.

### 3.3 Training Loss

Uses **Bayesian Advantage-Weighted PPO**:

$$
\mathcal L(¬µ,œÉ)=\mathbb E_{Œ∏‚àº\mathcal N(¬µ,œÉ^2)}\Big[\mathcal L_\text{clip}(Œ∏)+c_1¬∑\text{MSE}(V_Œ∏,R)+c_2¬∑\text{entropy}(œÄ_Œ∏)\Big]‚àíŒ≤_{KL}¬∑KL\big(\mathcal N(¬µ,œÉ^2)||\mathcal N(¬µ_0,œÉ_0^2)\big)
$$

This includes:
- Clipped surrogate objective (`PPO`)
- Value function loss (`MSE`)
- Entropy bonus for exploration
- KL divergence regularization (Bayesian prior)

Gradients use **reparameterization trick**.

### 3.4 Online Few-Shot Adaptation

When a new batch arrives:
1. Collect a **small rollout** (‚â§ 2√óH steps)
2. Update the Bayesian parameters (*¬µ, œÉ*) using one ELBO gradient step
3. Takes **<10ms** on an ARM-based edge device

This ensures rapid adaptation to new workloads without full retraining.

---

## 4. Adaptive Batch Scheduler (ABS) via Genetic Programming

### What It Does:

ABS evolves **program trees** that take `(task_features, node_features)` and output `(node_id, slot, DVFS_level)`.

- **Terminals:** e.g., `fog_load`, `task_MI`, `earliest_slot`, `œÅ`
- **Operators:** {+, ‚àí, √ó, √∑, min, max, if-less}

Each evolved tree represents a **scheduling strategy**.

### Initialization:

Population initialized so that **fog preference = œÅ** from BDRL ‚Üí cross-layer consistency.

### Fitness Function:

$$
F = E_\text{total} + Œª¬∑\text{lateness} + Œæ¬∑\text{variance}(node\_energy)
$$

This balances:
- Energy usage
- Deadline violations
- Load balancing across nodes

---

## 5. Pseudo-Code Suite

### 5.1 BDRL-PPO (Offline Training)

```text
Algorithm BDRL_Train(trace, ¬µ‚ÇÄ, œÉ‚ÇÄ)
1  initialise ¬µ ‚Üê ¬µ‚ÇÄ, œÉ ‚Üê œÉ‚ÇÄ
2  for each episode = one historical batch do
3      Rollout policy œÄ_{¬µ,œÉ} for T steps (sample Œ∏ ‚àº N(¬µ,œÉ¬≤))
4      Compute advantages √Ç via GAE
5      Update ¬µ ‚Üê ¬µ + Œ∑¬∑‚àá_¬µ L(¬µ,œÉ)   // Adam optimiser
6      Update œÉ ‚Üê œÉ + Œ∑¬∑‚àá_œÉ L(¬µ,œÉ)
7  end for
8  save ¬µ,œÉ as prior
```

**What it does:** Trains the Bayesian DRL policy using historical batches. Updates both mean and variance of the weight distribution.

---

### 5.2 BDRL On-Line Adaptation (per arriving batch)

```text
Algorithm BDRL_Adapt(cur_state, ¬µ, œÉ)
1  Sample Œ∏ ‚àº N(¬µ, œÉ¬≤); act œÅ ‚Üê œÄ_Œ∏(cur_state)
2  Execute œÅ for horizon H; collect tuple (s,a,r,s')
3  Compute one-step ELBO gradients g_¬µ, g_œÉ
4  ¬µ ‚Üê ¬µ + Œ∑_adapt¬∑g_¬µ; œÉ ‚Üê œÉ + Œ∑_adapt¬∑g_œÉ
5  return œÅ, ¬µ, œÉ
```

**What it does:** Adapts the Bayesian model in real-time based on current state and feedback from execution.

---

### 5.3 ABS Genetic Programming Loop

```text
Algorithm ABS_Evolve(G, œÅ, PopSize N, Gen G_max)
1  P ‚Üê Initialise(N, bias_fog=œÅ)
2  for g = 1‚Ä¶G_max do
3      for œá in P do
4          œá ‚Üê DVFS_Repair(œá)
5          fit(œá) ‚Üê EnergySim(G, œá)
6      end
7      P ‚Üê Tournament_Select(P)
8      P ‚Üê Crossover_Mutate(P)
9  end
10 return argmin_œá fit(œá)
```

**What it does:** Evolves a population of scheduling strategies (trees), evaluates them, selects the best, and evolves new ones over generations.

---

### 5.4 DVFS-Aware Repair

```text
DVFS_Repair(œá)
for task v in œá:
    node ‚Üê x_v; slot ‚Üê t_v
    choose lowest DVFS level l such that finish_time ‚â§ Œî·¥Æ
    if not feasible: migrate v to fastest node in F‚à™C
return œá
```

**What it does:** Ensures every scheduled task respects the deadline by choosing appropriate DVFS levels. If not possible, migrates to faster node.

---

### 5.5 Energy Simulator Stub

```text
EnergySim(G, œá)
E_total ‚Üê 0; lateness ‚Üê 0
for each task v in topological order:
    calc start, finish according to œá & predecessors
    P ‚Üê P_busy^{x_v}(l_v); E_total += P * (finish-start)
    update node timeline
lateness = max(0, max_finish - Œî·¥Æ)
return E_total + Œª*lateness + Œæ*variance(node_energy)
```

**What it does:** Simulates the execution of a schedule, computes energy, checks deadline, adds penalties, returns fitness value.

---

### 5.6 ASSEMBLY Master Scheduler

```text
Scheduler_Run(batch G, ¬µ, œÉ)
1  cur_state ‚Üê telemetry()
2  œÅ, ¬µ, œÉ ‚Üê BDRL_Adapt(cur_state, ¬µ, œÉ)
3  œá* ‚Üê ABS_Evolve(G, œÅ, N=30, G_max=100)
4  enact œá* on runtime orchestrator
5  record energy stats for next batch
```

**What it does:** Coordinates the whole workflow:
1. Gets system telemetry
2. Uses BDRL to adapt fog-cloud preference
3. Runs genetic programming to evolve best schedule
4. Executes the schedule
5. Records results for future learning

---

## 6. Variable Glossary (Complete)

| Symbol | Meaning | Units |
|--------|---------|-------|
| *G=(V,E)* | Batch workflow DAG | ‚Äì |
| *v,u* | Task indices | ‚Äì |
| *w·µ•* | CPU work of task | MI |
| *e·µ•* | Data size | MB |
| *S‚Çô(l)* | MIPS at DVFS level *l* | MI/s |
| *P·µ¢‚Çô, P_b‚Çô(l)* | Idle & busy power draw | W |
| *œÅ* | Fog preference ratio | ‚Äì |
| *¬µ,œÉ¬≤* | Mean & variance of Bayesian policy weights | ‚Äì |
| *Œ∏* | Concrete weight sample | ‚Äì |
| *H* | Horizon length for BDRL action | tasks |
| *Œ±,Œ≤* | Reward weights (energy, lateness) | ‚Äì |
| *Œª,Œæ* | ABS fitness weights | ‚Äì |
| *Œ∑,Œ∑_adapt* | Learning rates (train, adapt) | ‚Äì |
| *N, G_max* | ABS population & generation counts | ‚Äì |
| *l·µ•* | DVFS level for task *v* | index |
| *Œî·¥Æ* | Soft deadline for batch | s |

---

## 7. Dataset & Evaluation Details

- **Dataset:** Alibaba Cluster Trace 2018
  - Generate 14 workload sizes from 100 to 3000 tasks
- **Simulator:** iFogSim 2 or cloudsim extended with:
  - Per-node DVFS
  - Bayesian policy plugin

- **Metrics:**
  - Total energy (E_total)
  - Makespan
  - QoS violation %
  - Adaptation time
  - Policy uncertainty (avg œÉ)

---


## 9. Deliverables

- **Source Code:** PyTorch + Java implementation with Makefile
- **Documentation:** README, API docs, this design doc
- **Visualizations:** Plots of energy curves, uncertainty evolution, convergence

